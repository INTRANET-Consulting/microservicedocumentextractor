{"file_contents":{"README.md":{"content":"## Document Content Extractor Microservice\n\n### Overview\nA dedicated microservice for extracting text content from various document formats using the unstructured library. \n\n### CURL example\n```bash\ncurl -X POST \"http://localhost:8002/process\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"files=@/path/to/document.pdf\"\n```\n","size_bytes":329},"pyproject.toml":{"content":"[project]\nname = \"document-content-extractor\"\nversion = \"0.1.0\"\ndescription = \"Microservice for extracting content from documents using unstructured\"\nauthors = []\ndependencies = [\n    # Web framework\n    \"fastapi>=0.104.1\",\n    \"uvicorn>=0.24.0\",\n    \"python-multipart>=0.0.6\",\n    \n    # Document processing\n    \"unstructured[all-docs]\",\n    \"pdfplumber>=0.10.3\",  # Override old dependency\n    \"python-magic>=0.4.27\",\n    \n    # Utilities\n    \"python-dotenv>=1.0.0\",\n    \"pydantic>=2.5.2\",\n    \"pydantic-settings>=2.1.0\",\n]\n# readme = \"README.md\"  # Commented out until README is created\nrequires-python = \">= 3.11\"\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.rye]\nmanaged = true\ndev-dependencies = [\n    \"pytest>=7.4.3\",\n    \"httpx>=0.25.2\",\n]\n\n[tool.rye.scripts]\ndev = \"uvicorn src.main:app --host 0.0.0.0 --port 8002\"\nstart = \"uvicorn src.main:app --host 0.0.0.0 --port 8002\"\ntest = \"pytest tests/\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src\"]\n\n[[tool.rye.sources]]\nname = \"pytorch\"\nurl = \"https://download.pytorch.org/whl/cpu\"\ntype = \"index\"","size_bytes":1093},"src/__init__.py":{"content":"","size_bytes":0},"src/main.py":{"content":"from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nimport logging\nimport gc\nfrom contextlib import asynccontextmanager\nfrom .settings import get_settings\nfrom .routes import router\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Lifecycle manager for the FastAPI application.\n    Handles startup and shutdown events.\n    \"\"\"\n    # Startup\n    logger.info(\"Starting up Document Content Extractor service...\")\n    gc.collect()  # Initial garbage collection\n    \n    try:\n        yield\n    finally:\n        # Shutdown\n        logger.info(\"Shutting down Document Content Extractor service...\")\n        # Perform cleanup\n        gc.collect()\n        logger.info(\"Cleanup completed\")\n\ndef create_application() -> FastAPI:\n    \"\"\"Create and configure FastAPI application\"\"\"\n    settings = get_settings()\n    \n    app = FastAPI(\n        title=\"Document Content Extractor\",\n        description=\"Microservice for extracting content from documents using unstructured\",\n        lifespan=lifespan  # Add lifecycle management\n    )\n    \n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    \n    # Include routes\n    app.include_router(router)\n    \n    return app\n\napp = create_application()\n\n# Add startup and shutdown event handlers for additional cleanup\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Additional startup tasks\"\"\"\n    logger.info(\"Performing additional startup tasks...\")\n    # Clear any temporary files that might have been left from previous runs\n    # You might want to add cleanup of specific directories here\n    \n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Additional shutdown tasks\"\"\"\n    logger.info(\"Performing additional shutdown tasks...\")\n    # Additional cleanup tasks\n    gc.collect()  # Final garbage collection","size_bytes":2055},"src/models.py":{"content":"from pydantic import BaseModel\nfrom typing import Optional, List\n\nclass ProcessedFile(BaseModel):\n    \"\"\"Information about processed file\"\"\"\n    filename: str\n    file_type: str\n    status: str\n    error: Optional[str] = None\n\nclass ProcessingResponse(BaseModel):\n    \"\"\"API response for document processing\"\"\"\n    content: str\n    processing_info: List[ProcessedFile]","size_bytes":368},"src/processor.py":{"content":"import logging\nimport os\nfrom typing import List, Tuple\nfrom fastapi import UploadFile\nfrom unstructured.partition.auto import partition\nimport magic\nimport tempfile\nfrom contextlib import contextmanager\nimport gc\nfrom .models import ProcessedFile\nfrom .settings import get_settings\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@contextmanager\ndef temporary_file(suffix: str = None):\n    \"\"\"Context manager for handling temporary files with proper cleanup\"\"\"\n    temp_file = None\n    try:\n        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n        yield temp_file\n    finally:\n        if temp_file:\n            temp_file.close()\n            try:\n                if os.path.exists(temp_file.name):\n                    os.unlink(temp_file.name)\n            except Exception as e:\n                logger.warning(f\"Failed to delete temporary file: {str(e)}\")\n\nclass DocumentProcessor:\n    \"\"\"Handles document processing with memory management\"\"\"\n    \n    @staticmethod\n    async def validate_file(file: UploadFile) -> Tuple[str, int]:\n        \"\"\"Validate file size and type\"\"\"\n        settings = get_settings()\n        content = None\n        try:\n            content = await file.read()\n            file_size = len(content)\n            \n            if file_size > settings.max_file_size:\n                raise ValueError(f\"File size exceeds limit of {settings.max_file_size} bytes\")\n\n            mime_type = magic.Magic(mime=True).from_buffer(content)\n            logger.info(f\"Detected MIME type: {mime_type}\")\n            \n            return mime_type, file_size\n        finally:\n            if content:\n                del content\n                gc.collect()\n            await file.seek(0)\n\n    @staticmethod\n    async def extract_text(file: UploadFile, mime_type: str) -> str:\n        \"\"\"Extract text content from file using unstructured\"\"\"\n        content = await file.read()\n        \n        try:\n            with temporary_file(suffix=os.path.splitext(file.filename)[1]) as temp_file:\n                temp_file.write(content)\n                temp_file.flush()\n                \n                elements = partition(filename=temp_file.name)\n                text_content = \"\\n\".join([str(element) for element in elements])\n                \n                logger.info(f\"Extracted {len(text_content)} characters from {file.filename}\")\n                return text_content\n                \n        finally:\n            del content\n            gc.collect()\n            await file.seek(0)\n\n    @staticmethod\n    async def process_single_file(file: UploadFile) -> Tuple[str, ProcessedFile]:\n        \"\"\"Process a single file with proper resource cleanup\"\"\"\n        logger.info(f\"Processing file: {file.filename}\")\n        \n        try:\n            mime_type, _ = await DocumentProcessor.validate_file(file)\n            text_content = await DocumentProcessor.extract_text(file, mime_type)\n            \n            processed_file = ProcessedFile(\n                filename=file.filename,\n                file_type=mime_type,\n                status=\"success\"\n            )\n            \n            return text_content, processed_file\n            \n        except Exception as e:\n            logger.error(f\"Failed to process {file.filename}: {str(e)}\")\n            processed_file = ProcessedFile(\n                filename=file.filename,\n                file_type=getattr(file, 'content_type', 'unknown'),\n                status=\"error\",\n                error=str(e)\n            )\n            return \"\", processed_file\n        finally:\n            gc.collect()\n\n    @staticmethod \n    async def process_files(files: List[UploadFile]) -> Tuple[str, List[ProcessedFile]]:\n        \"\"\"Process multiple files with memory management\"\"\"\n        logger.info(f\"Processing {len(files)} files\")\n        \n        processed_contents = []\n        processing_info = []\n        \n        try:\n            for file in files:\n                content, info = await DocumentProcessor.process_single_file(file)\n                if content:\n                    processed_contents.append(content)\n                processing_info.append(info)\n            \n            combined_content = \"\\n\\n\".join(processed_contents)\n            logger.info(f\"Processed {len(files)} files, total content length: {len(combined_content)}\")\n            \n            return combined_content, processing_info\n        finally:\n            # Clean up after processing\n            for file in files:\n                await file.close()\n            gc.collect()","size_bytes":4577},"src/routes.py":{"content":"from fastapi import APIRouter, UploadFile, File, HTTPException\nfrom typing import List\nimport logging\nfrom .processor import DocumentProcessor\nfrom .models import ProcessingResponse\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter()\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"\n    Check service health status\n    \n    Returns:\n        dict: Service health information\n    \"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"document-content-extractor\",\n        \"version\": \"0.1.0\"\n    }\n\n@router.post(\"/process\", response_model=ProcessingResponse)\nasync def process_documents(\n    files: List[UploadFile] = File(...),\n) -> ProcessingResponse:\n    \"\"\"\n    Process documents and extract their content.\n    \n    Args:\n        files: List of files to process\n        \n    Returns:\n        ProcessingResponse with extracted content and processing info\n    \"\"\"\n    try:\n        if not files:\n            raise HTTPException(\n                status_code=400,\n                detail=\"No files provided\"\n            )\n        \n        content, processing_info = await DocumentProcessor.process_files(files)\n        \n        return ProcessingResponse(\n            content=content,\n            processing_info=processing_info\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error processing documents: {str(e)}\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error processing documents: {str(e)}\"\n        )\n    finally:\n        # Cleanup\n        for file in files:\n            await file.close()","size_bytes":1634},"src/settings.py":{"content":"from pydantic_settings import BaseSettings\nfrom functools import lru_cache\n\nclass Settings(BaseSettings):\n    # Server settings\n    port: int = 5000\n    \n    # Processing settings\n    max_file_size: int = 10 * 1024 * 1024  # 10MB\n\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n\n@lru_cache()\ndef get_settings() -> Settings:\n    return Settings()","size_bytes":409}},"version":1}